# 逻辑回归算法浮点数定点化解析及优化
>  根据12月23号的圆桌会上专家提出了1.6版LR的性能优化方向，追溯回源码作了解析、尝试对源码进行了修改和测试，还请大佬们多多指正。
## 1.LR浮点数定点化解析


 将通过如图所示，在纵向LR信息交互过程中，根据残差d，A 和方和B方可以算出局部加密梯度
 
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/2020122815582113.png)
 
 其中d为密文，xa和xb皆为明文。除去硬件方面的加速方式，算法方面可以考虑半精度计算的形式。
 
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020122815494682.png)

### 1.1浮点数定点化解析
先给大家介绍下为何浮点数定点化会加快计算速度。
#### 1） 浮点数：小数点位置是漂浮不定的。
例如1.1 * 1.1=1.21 ，小数点位置发生了改变。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201228170528863.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201228174207281.png)

绿色整数部分01111100二进制转十进制为124，标准阶码为127位，真正阶码为124-127=-3，红色部分是小数位，因为保存尾数的时候省略了小数点前面的1，所以要加回去，M=1.01，计算出来1.01*2^-3=0.00101，转十进制变成0.15625

#### 2）定点数：小数点的位置是确定的。
例如 1.1 * 1.1 = 1.2，小数点的位置没有变化。
整数部分01111100二进制转十进制为124
二进制小数的阶数为负数，
即01000000000000000000000=1*2（-2）=-0.25
因此结果为124.25

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201228163638347.png)

* 定点的意思是，小数点固定在 32 位中的某个位置，前面的是整数，后面的是小数。

>浮点数定点化演示：
2.918 * 3.1415926 = 9.1671672068，尝试将浮点数进行定点化。
1、定点转换（Qn=12）
符号1位，整数取3位（实际2位就够），小数12位，可以看成把 1 分成了 2^12份，因此：
round(2.918  * 2^12 )       = round(11952.168     )    = 11952；
round(3.1415926 * 2^12 )= round(12867.8632896) = 12868；
2、定点数相乘
　　11952 * 12868 = 153798336。
3、结果还原
　　相乘后，整数部分为 6 位，小数部分为 24 位。
　　因此结果 = 153798336 / 224 = 9.167095184326171875，和原计算值9.1671672068差距非常小。

### 1.2 追溯源码
* 追溯到源码正是利用了这个原理，缩短计算的位数从而提升了速度

>源码路径：
>python\federatedml\secureprotol\fate_paillier.py
>python\federatedml\secureprotol\fixedpoint.py

由于该步采用的加密方法为paillier加密，
首先追溯到fate_paillier.py中类PaillierEncryptedNumber，再追溯到位于fixedpoint.py的类FixedPointNumber

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201228161345175.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201228161809466.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201228161953269.png)

在类FixedPointNumber中，26-28行定义了

BASE  | 16
-------- | -----
LOG2_BASE| 4
FLOAT_MANTISSA_BITS  | 54
再追溯到类FixedPointNumber中encode函数

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201228175337196.png)![在这里插入图片描述](https://img-blog.csdnimg.cn/20201228175123994.png)

>frexp函数表示：frexp(x)=(m,n)   其中x=m*2^n

scalar表示明文x，
66行求得阶数n
67行将默认系统阶数53-n
68行实际定点化阶数为(53-n)/4
78行表示定点化过程round（x*2^((53-n)/4))
###  1.3 模拟验证理解
假设scalar即明文x为0.1234567，从下图可看出第8行即按源码68行做一个除以4的操作，缩短了int_fixpoint的位数，且不影响还原为原值0.1234567。

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020122817573880.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201228175950315.png)

## 2.LR定点化优化

### 2.1模拟测试
将x定点化为round(x*2^23),将浮点数化为整数，则精度为保留6-7位小数，最终decode的时候再除以2 ^23 。
如下图所示，假设scalar即x为0.12345678999定点化后的结果还原为
0.12345683574676514，精度可保留到6位小数，但int_fixpoint位数相比源码缩短明显。

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020122819040277.png)

### 2.2源码修改
基于模拟测试的验证，对源码fixedpoint.py进行修改。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201228191210152.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201228191421101.png)

>1.将原26-28行注释，base赋值为2
2.将原67-69行注释，exponent赋值为23

这样修改后，80行的定点化则为round(scalar*2^23)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201228191515254.png)


### 2.3 测试对比
同一份数据按源码去跑纵向lr模型，每迭代一次的耗时为6分25秒

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201228192229248.png)


修改后每迭代一次耗时为4分30秒，计算提速31%左右

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201228191853138.png)

