# 横向模型参数聚合解析及代码优化
## 一、理论介绍
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200818152932869.png)

横向训练分为梯度平均和模型平均两种，Fate源码中用的为模型平均
#### 梯度平均：
第一步：各参与方在本地计算模型梯度，对梯度信息进行加密后发送给聚合服务器（arbiter）
第二步：聚合服务器进行安全聚合（secure aggregation）操作，将梯度进行加权平均计算
第三步：聚合服务器将聚合后的结果发给各参与方
第四步：各参与方根据获得的聚合结果进行解密并更新自方模型参数
上述步骤将会持续迭代进行，直到损失函数收敛或者达到允许的迭代次数上限
#### 模型平均：
第一步：各参与方在本地计算模型参数和损失以及己方行数，对信息进行加密后发送给聚合服务器（arbiter）
第二步：聚合服务器进行安全聚合（secure aggregation）操作，将模型参数运用两方行数进行加权平均计算
第三步：聚合服务器将聚合后的模型参数结果发给各参与方
第四步：各参与方根据获得的聚合结果进行解密后更新自方模型参数
上述步骤将会持续迭代进行，直到损失函数收敛或者达到允许的迭代次数上限

## 二、代码解析
>根据源码及打印日志得出以下结论
> 源码路径:federatedml\optim\optimizer.py
### 1.guest 及host方在本地计算出模型参数
* Guest方：
用sgd计算出delta_grad，其中learning_rate会随着迭代次数的增多而衰减【learning_rate=0.15/sqrt(iter)】
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914100839807.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914100904159.png)

根据举例演算可得出符合公式：
delta_grad=learning_rate*grad![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914101451258.png)

其中grad 是通过运算grad=grad+alpha*model_weights
即 delta_grad=(grad+alpha*model_weights) * learning_rate
但敲重点：88行表明权重才进行这样的计算，
截距是直接delta_grad=grad * learning_rate 
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200915143344252.png)
>举例：alpha默认配置为0.01
按公式
delta=(0.11238168+0.01*(-0.12720372))*0.106066017177982=0.011784957
与日志中打印出来的结果一致~
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200916113746646.png)
 Host方同理
 ### 2.arbiter方聚合模型参数
* Arbiter方对接受到了双方参数进行模型加权平均：
公式为：
(guest方模型参数*guest方degree+host方模型参数*host方degree)/(guest方degree+host方degree），degree表示双方各自的行数
举例说明：(-0.07687309*227+-9.28687105e-02*228)/(227+228)=-0.0848879777665457
>guest方日志：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914111559805.png)
>host方日志：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914111624426.png)
### 3.guest及host方在本地更新模型参数
> 双方收到arbiter发来的优化模型参数后
> （iters=n-1前）
* 用公式模型参数=arbiter方发来的优化模型参数-delta_grad
> 双方收到arbiter发来的优化模型参数后
> （iters=n，即最后一次迭代时）
* 用公式模型参数=arbiter方发来的优化模型参数

* 举例说明:
>guest 方日志：
获得arbiter传来的聚合后模型参数为-0.10469419761723547，再减去对应的delta_grad(learning_rate*grad)：0.0165299,求得模型参数-0.10469419761723547-0.0165299=-0.121224098

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914144205107.png#pic_center)![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914144319450.png)![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914144400626.png)
* host 方同理
## 三、代码优化
> 根据源码跑出的guest方与host方模型参数一样，arbiter参数与双方不同，如下所示：
> guest方:
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914152654105.png)
host 方:
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914152837476.png)
arbiter方:
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914152933137.png)
-------
基于理论角度考虑，最终三方参数应当相同，故追溯回源码及打印的日志，观察到arbiter方之所以不同是因为其在board界面获取到的是倒数第二次迭代的参数结果（如下图所示，本次任务max_n_iters:50)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914162602384.png)
arbiter方第50次迭代的结果与guest方和host方的最终结果一致
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020091416270638.png)
因此，追溯到源码\federatedml\linear_model\logistic_regression\homo_logsitic_regression\homo_lr_arbiter.py
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914163313443.png)
将77-78行移至79行后面，使其到了max_iter之前将最后一次参数再赋值于model_weights后再break(改为如下图所示）
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914163507782.png)
按如上修改源码后再提交任务可观察到三方的模型参数就是一致的了~
guest方：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914152654105.png)
host 方:
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914152654105.png)
arbiter方:
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200914152654105.png)








