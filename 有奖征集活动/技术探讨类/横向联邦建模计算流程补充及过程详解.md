

## 一、主要补充内容

之前小伙伴的pr中已经对横向联邦的计算过程有了详细解析，横向联邦的guest和host方在本地通过公式w_i=w_i-1 - delta_grad更新模型后，会将权重w一起发送给arbiter方进行聚合，聚合方式为根据样本数量大小进行加权平均，但是在实际操作中发现，本地更新后的权重与发送给arbiter方的权重有差异，以下面guest方第一个权重计算结果为例，本地更新模型后的权重为-0.08857158-0.01579847 = -0.10434004，与传递给arbiter方的权重before aggregate中第一个权重-0.1030457不一致：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201023145855628.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9sYWppYW82NDE=,size_16,color_FFFFFF,t_70#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201023145910421.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9sYWppYW82NDE=,size_16,color_FFFFFF,t_70#pic_center)

host方也同理，说明guest方和host方在本地更新完权重发送给arbiter方之前还进行了其他的计算，通过查看源码发现，在federatedml\framework\homo]procedure\aggregator.py 中定义了安全聚合函数，让权重在发送前进行加密，加密过程中federatedml\secureprotol\encrypt.py 中定义了一个加密函数，在权重传递给arbiter前加入噪声，计算公式为w = w±U(0,1)（U(0,1)为0-1的随机数），由于后续arbiter方聚合时权重又会加权平均，所以实际权重加的噪音变动应该为±U(0,1) * /n_k（n_k为guest方与host方总的样本数量和），根据我的理解这种加密方式应该是属于差分隐私算法中的算法扰动，保证权重微小变化不影响模型性能的情况下防止信息泄露
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201023145958182.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9sYWppYW82NDE=,size_16,color_FFFFFF,t_70#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201023150007411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9sYWppYW82NDE=,size_16,color_FFFFFF,t_70#pic_center)

 

## 二、横向联邦计算过程详解

因此重新对横向联邦计算过程在进行详细的梳理和解析，由于guest方与host方计算过程一致，下面以guest方为例。
**1、初始化权重**
先初始化一个截距不为0，其余均为0的权重w`：
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201023150046318.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9sYWppYW82NDE=,size_16,color_FFFFFF,t_70#pic_center)

**2、本地进行权重和参数更新计算**
本次训练将样本分为3个batch，对于每一个batch，先根据迭代次数更新学习率learning_rate=n/sqrt(1+decay*iter)，（decay为初始化参数，默认为1，iter为当前迭代次数，n为初始设置的学习率，默认为0.15），代码位于fedaratedml\optim\optimizer.py
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201023150057483.png#pic_center)

根据样本计算梯度new_grad=(grad+α * w_i)，在计算delta_grad =learning_rate*new_grad （α为初始设置的参数，默认0.01，目的应该是做移动平均，防止梯度下降过快，截距项梯度不做此变换，直接乘学习率)，
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201023150231661.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9sYWppYW82NDE=,size_16,color_FFFFFF,t_70#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020102315024264.png#pic_center)

 
以第一个权重为例，由于是第0次迭代，因此learning_rate = 0.15/sqrt(1)=0.15，delta_grad=(0.39357968+0.01 * 0)*0.15=0.05903695
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201023150252936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9sYWppYW82NDE=,size_16,color_FFFFFF,t_70#pic_center)

**3、更新权重w并在batch之间进行权重传递和更新**
根据公式w_i= w_i-1 -delta_grad更新本地参数并传递给下一个batch作为初始权重，以第一个权重为例，-0.05903695=0 - 0.05903695，重复2-3步骤直至最后一个batch计算出权重矩阵w（若只有一个batch则无需传递，更新本地权重后直接进入第4步），代码位于federatedml\linear_model\logistic_regression\homo_logsitic_regression\homo_lr_guest.py（本例以逻辑回归为例）
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201023150316507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9sYWppYW82NDE=,size_16,color_FFFFFF,t_70#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201023150322775.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9sYWppYW82NDE=,size_16,color_FFFFFF,t_70#pic_center)

 
**4、最后一个batch进行本地计算加密传递给arbiter**
最后一个batch计算权重后会传递给arbiter方进行聚合计算，按照2-3步骤计算，以第一个权重为例-0.08857158-0.01579847 = -0.10434004，在传递给arbiter方之前会进行两步操作，第一步为w = w * degree（degree为样本数量），第二部为加密操作，在权重传递给arbiter前加入噪声，计算公式为w = w±U(0,1)（U(0,1)为0-1的随机数）
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/2020102315035269.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9sYWppYW82NDE=,size_16,color_FFFFFF,t_70#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201023150358541.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9sYWppYW82NDE=,size_16,color_FFFFFF,t_70#pic_center)

 
**5、arbiter接受双方权重进行聚合并传回**
arbiter方接收到guest和host的w和degree后（w为权重，degree为样本数量），按照样本数量大小进行加权平均，计算公式为w=(n_1 * w_1+n_2 * w_2)/n，（n_1和n_2分别为双方的样本量，n_1+n_2=n），以第一个权重为例，-0.1114995339206302 = (-0.1030457 * 227+-0.119916286 * 228)/455，权重聚合完成后发回给guest和host方，重复上述迭代直至收敛或者达到最大迭代次数，代码位于federatedml\framework\homo\procedure\aggregator.py
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201023150424598.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9sYWppYW82NDE=,size_16,color_FFFFFF,t_70#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201023150431724.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9sYWppYW82NDE=,size_16,color_FFFFFF,t_70#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020102315044099.png#pic_center)

 
 


